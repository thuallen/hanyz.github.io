<!DOCTYPE HTML>
<!-- saved from url=(0034)http://www.gaohuang.net/ -->
<!DOCTYPE html PUBLIC "" ""><!-- Lab of GaoHuang from THU -->
<HTML lang="en-us"><HEAD><META content="IE=11.0000" http-equiv="X-UA-Compatible">
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<TITLE>Yizeng Han Homepage</TITLE>   
<LINK href="bootstrap.min.css" rel="stylesheet">  
<SCRIPT src="jquery-3.1.1.slim.min.js"></SCRIPT>

<STYLE type="text/css">
        /* @import url("http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic"); */

        body {
            /* font-family:"Roboto",Helvetica,Arial,sans-serif; */
            /* font-family: Arial, Helvetica, sans-serif; */
            /* font-family: "Times New Roman",Georgia,Serif; */
            font-family: "Helvetica", Helvetica, sans-serif;
            font-size: 16px;
            line-height: 1.5;
            font-weight: normal;
            background-color: #ffffff;
        }

        .navigation {
            height: auto;
            width: 100%;
            margin-left: 0;
            background: #8F4B8D;
            opacity: 0.9;
            position: fixed;
            top: 0;
        }

        .navigation ul {
            width: auto;
            list-style-type: none;
            white-space: nowrap;
            margin-left: 22%;
            padding: 0;
        }

        .navigation li {
            float: right;
            text-align: center;
            line-height: 40px;
            margin-right: 2%;
            position: relative;
            overflow: hidden;
        }

        #name-nav {
            float: left;
            position: relative;
            display: block;
            color: white;
            text-align: center;
            padding: 3px;
            overflow: hidden;
            font-size: 120%;
            text-decoration: none;
        }

        .navigation li a,
        .navigation li span {
            display: block;
            color: white;
            text-align: center;
            padding: 3px;
            overflow: hidden;
            text-decoration: none;
        }

        .navigation li a:hover {
            text-decoration: underline;
        }

        .navigation li span:hover {
            text-decoration: underline;
            cursor: pointer;
        }

        b {
            font-weight: 600;
        }

        .content {
            width: 100%;
            padding-top: 4%;
            /* margin : 0px auto; */
            background-color: #ffffff;
        }

        table {
            padding: 5px;
        }

        table.pub_table,
        td.pub_td1,
        td.pub_td2 {
            padding: 8px;
            width: 850px;
            border-collapse: separate;
            border-spacing: 15px;
            margin-top: -5px;
        }

        td.pub_td1 {
            width: 50px;
        }

        td.pub_td1 img {
            height: 120px;
            width: 160px;
        }

        div#container {
            padding-left: 20px;
            padding-right: 20px;
            margin-left: 15%;
            margin-right: 15%;
            /* width: 860px; */
            text-align: left;
            /* position: relative; */
            background-color: #FFF;
        }

        div#portrait {
            /* color: #1367a7;
        color: rgb(34, 110, 147); */
            height: 158px;
        }

        #portrait {
            /* position: relative; */
            /*margin-left: 100%;*/
        }

        h4,
        h3,
        h2,
        h1,
        .paperlo,
        .paperhi-only {
            color: rgb(153,87,157);
        }

        .text_container h2:hover {
            cursor: pointer;
        }

        .paperhi-only,
        .paperlo:hover {
            cursor: pointer;
        }

        h2 {
            font-size: 130%;
        }

        #paper-show {
            color: rgb(0, 85, 170);
            font-size: 80%;
        }

        #paper-show span:hover {
            text-decoration: underline;
        }

        /* p
	{
		color: #5B5B5B;
		margin-bottom: 50px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
	} */

        #header_img {
            position: absolute;
            top: 0px;
            right: 0px;
        }

        /* 
        a:link,
        a:visited {
            color: #1367a7;
            text-decoration: none;
        } */

        #mit_logo {
            position: absolute;
            left: 646px;
            top: 14px;
            width: 200px;
            height: 20px;
        }

        table.pub_table tr {
            outline: thin dotted #666666;
        }

        .paper-title {
            color: black;
            text-decoration: none;
        }
        .img_box{
            width: 300px;
            height: 300px;
            float: left;   
        }

        .papericon {
            /* border-radius: 8px; */
            /* -moz-box-shadow: 3px 3px 6px #888;
            -webkit-box-shadow: 3px 3px 6px #888;
            box-shadow: 3px 3px 6px #888; */
            /*height: 120px;*/
            width: 270px;
        }

        .media {
            margin-bottom: 15px;
            margin-left: 10px;
        }

        .media-body {
            margin-top: 5px;
            display: inline;
        }

        .publication {
            margin-bottom: 20px;
        }

        .papers-selected .publication {
            display: none;
        }

        .papers-selected .book-chapters {
            display: none;
        }

        .papers-selected #show-selected {
            color: black;
            text-decoration: underline;
        }

        .papers-selected .paperhi {
            display: flex;
        }

        .papers-selected .paper-year {
            display: none;
        }

        .papers-by-date #show-by-date {
            color: black;
            text-decoration: underline;
        }

        .papers-by-date .paper-selected {
            display: none;
        }

        .papers-by-date .book-chapters {
            display: none;
        }

        .book-chapters #book-chapters {
            color: black;
            text-decoration: underline;
        }

        .book-chapters .paper-selected,
        .book-chapters .paper-year,
        .book-chapters .publication {
            display: none;
        }

        .book-chapters .chapter {
            display: flex;
        }

        /* .papers-by-date .paperhi {
            display: none;
        } */

        .hidden>div {
            display: none;
        }

        .visible>div {
            display: block;
        }
        .clear {
            clear: both;
        }
</STYLE>
     
<SCRIPT>
        $(document).ready(function () {
            $('#show-selected').click(function () {
                $('.papers-container').removeClass('papers-by-date');
                $('.papers-container').removeClass('book-chapters');
                $('.papers-container').addClass('papers-selected');
            });

            $('#show-by-date').click(function () {
                $('.papers-container').removeClass('papers-selected');
                $('.papers-container').removeClass('book-chapters');
                $('.papers-container').addClass('papers-by-date');
            });

            $('#book-chapters').click(function () {
                $('.papers-container').removeClass('papers-selected');
                $('.papers-container').removeClass('papers-by-date');
                $('.papers-container').addClass('book-chapters');
            });

            $('.papers-container').addClass('papers-selected');


            $('.text_container').addClass("hidden");
            $('.text_container').click(function () {
                var $this = $(this);

                // if ($this.hasClass("hidden")) {
                //     $(this).removeClass("hidden").addClass("visible");

                // } else {
                //     $(this).removeClass("visible").addClass("hidden");
                // }
            });

            // document.querySelector("#news-nav").onclick = function () {
            //     document.querySelector("#news").scrollIntoView({
            //         block: "center",
            //         behavior: "smooth"
            //     });
            // }
            // publications
            document.querySelector("#publication-nav").onclick = function () {
                document.querySelector(".paperlo").scrollIntoView({
                    block: "center",
                    behavior: "smooth"
                });
            }
            // awards
            document.querySelector("#award-nav").onclick = function () {
                document.querySelector("#award").scrollIntoView({
                    block: "center",
                    behavior: "smooth"
                });
            }
            // talks
            document.querySelector("#talk-nav").onclick = function () {
                document.querySelector("#talk").scrollIntoView({
                    block: "center",
                    behavior: "smooth"
                });
            }
            // students
            document.querySelector("#student-nav").onclick = function () {
                document.querySelector("#student").scrollIntoView({
                    block: "center",
                    behavior: "smooth"
                });
            }
            // more
            document.querySelector("#more-nav").onclick = function () {
                document.querySelector("#more").scrollIntoView({
                    block: "center",
                    behavior: "smooth"
                });
            }

        });
</SCRIPT>
 
<META name="GENERATOR" content="MSHTML 11.00.10570.1001">

    <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?85819d0bd906cafe9ed67ce601c2e750";
          var s = document.getElementsByTagName("script")[0]; 
          s.parentNode.insertBefore(hm, s);
        })();
    </script>

</HEAD>

<BODY id="top">
<DIV class="navigation">
<UL>
<!--  <LI id="name-nav"></LI>
 <LI id="more-nav"><SPAN>More</SPAN></LI>
  <LI id="talk-nav"><SPAN>Talks</SPAN>
  <LI>
  <LI id="award-nav"><SPAN>Awards</SPAN>
  <LI> -->
  <li class="nav-item"><a href="#contact">Contact</a></li>
  <!-- <li class="nav-item"><a href="#talks">Talks</a></li> -->
  <!--<li class="nav-item"><a href="#students">Students</a></li> -->
  <li class="nav-item"><a href="#awards">Awards</a></li>
  <li class="nav-item"><a href="#publications">Publications</a></li>

  <li class="nav-item"><a href="#top">Home</a></li>

  <!-- <li class="nav-item"><a href="http://www.gaohuang.net/#teaching">Teaching</a></li> -->
  <!-- <li class="nav-item"><a href="http://www.gaohuang.net/#miscellaneous">Misc</a></li> -->

 <!-- <LI id="contact-nav"><SPAN>Contact</SPAN>
  <LI>
  <LI id="publication-nav"><SPAN>Publications</SPAN></LI>
  <LI><A href="http://www.gaohuang.net/#top">Home</A></LI> -->
</UL>
</DIV>

<DIV class="content">
<DIV id="container">
<TABLE width="100%">
  <TBODY>
  <TR>
    <TD width="60%">
      <DIV id="info" width="">
      <H1><br>Yizeng Han</H1><br>
      <P><B>Ph.D Candidate</B>, advised by Prof. <a href="http://www.gaohuang.net/">Gao Huang</a> and Prof. <a href="http://www.au.tsinghua.edu.cn/info/1075/1590.htm">Shiji Song</a>.<br>Department of Automation, Tsinghua University.</P>
      <h2>Education</h2>
      <Li>Ph.D, Tsinghua University, 2018 - present.</Li><Li>B.E., Tsinghua University, 2014 - 2018.</Li>
      <br>
    </DIV>
    </TD>
    <TD width="40%">
      <DIV id="photo" style="margin-left: 15%; float: right;"><IMG height="200" 
      id="portrait" src="figures/author_photo_yizenghan.jpg"></DIV>
    </TD>
  </TR>
  </TBODY>
</TABLE>

<h2>Research Experience</h2>
    <li>Intern, Georgia Institute of Technology, 06/2017 - 08/2017</li>
    <br>

<h2>Research Interest</h2>
    My research focuses on machine learning and computer vision, in particular deep learning, eﬃcient inference and dynamic neural networks.
    <br>
    <br>

<h2>News</h2>
    <li>10/2022: Awarded by National Scholarship, Ministry of Education of China.</li>
    <li>09/2022: Our work (latency-aware spatial-wise dynamic networks) is accepted by <b>NeurIPS</b> 2022. </li>
    <li>07/2022: Our work (learning to weight samples of dynamic early-exiting networks) is accepted by <b>ECCV</b> 2022. </li>
    <br>
    <br>

<a name="publications"></a>
<DIV class="papers-container">
    <H2 class="paperlo">Recent Publications &amp; Preprints (<a href="https://scholar.google.com/citations?user=25mubAsAAAAJ&hl=en">Google Scholar</a>)
        <br>
        <SPAN id="paper-show">
            <!-- (<SPAN id="show-selected">show selected</SPAN> /  
                <SPAN id="show-by-date">show all by date</SPAN> / 
                <SPAN id="book-chapters">book chapters</SPAN>) -->
        </SPAN>       
    </H2><!-- <h5 class="paperhi paperhi-only">Representative Publications</h5> --> 

    <DIV class="paper-selected">  
        <!-- (<a href="https://scholar.google.com/citations?user=-P9LwcgAAAAJ&amp;hl=en">Full publication list on Google Scholar</a>) -->
        <br>
        <DIV class="publication media paperhi">
            <DIV class="img_box"><IMG class="papericon" src="figures/survey.png"></DIV>
            <DIV class="media-body ">
                <A class="paper-title" href="https://arxiv.org/pdf/2102.04906.pdf"><B>Dynamic Neural Networks: A Survey.</B></A> [<A href="https://mp.weixin.qq.com/s/TG_HBAR7Jrec4X02sxNGEA"    target="_blank">智源社区</A>][<A href="https://jmq.h5.xeknow.com/s/2H6ZSj">机器之心-在线讲座</A>][<A href="https://www.bilibili.com/video/BV19B4y1A7Wy?from=search&seid=12254026542403915477">Bilibili</A>][<A href="papers/动态神经网络研究概述.pdf">slides</A>]
                <br/><b>Yizeng Han</b>*, Gao Huang*, Shiji Song, Le Yang, Honghui Wang, Yulin Wang.
                <br/><I>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b><font color="red">TPAMI</font></b>, IF=24.314), 2021</I>
                <br/>Dynamic neural network is an emerging research topic in deep learning. Compared to static models which have fixed computational graphs and parameters at the inference stage, dynamic networks can adapt their structures or parameters to different inputs, leading to notable advantages in terms of accuracy, computational efficiency, adaptiveness, etc. In this survey, we comprehensively review this rapidly developing area. The important research problems of dynamic networks, e.g., architecture design, decision making scheme, and optimization technique, are reviewed systematically. Finally, we discuss the open problems in this field together with interesting future research directions.
            </DIV>
            <DIV class="clear"></DIV>
        </DIV>
        
        <DIV class="publication media paperhi">
            <DIV class="img_box"><IMG class="papericon" src="figures/LASNet.jpg"></DIV>
            <DIV class="media-body ">
                <A class="paper-title" href="https://www.arxiv.org/pdf/2209.08310.pdf"><B>Latency-aware Spatial-wise Dynamic Networks.</B></A> [<A href="https://github.com/LeapLabTHU/LASNet"    target="_blank">code</A>]
                <br/><b>Yizeng Han*</b>, Zhihang Yuan*, Yifan Pu*, Chenhao Xue, Shiji Song, Guangyu Sun, Gao Huang.
                <br/><I>Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2022</I>
                <br/> In this paper, we propose to use a latency predictor to guide both the algorithm design and the scheduling optimization of spatial-wise dynamic networks on various hardware platforms. We show that "coarse-grained" spatially adaptive computation can effectively reduce the memory access cost and show superior practical efficiency than the conventional pixel-level dynamic operations. Experiments on image classification, object detection and instance segmentation demonstrate that the proposed framework significantly improves the practical inference efficiency of deep networks.
            </DIV>
            <DIV class="clear"></DIV>
        </DIV>

        <DIV class="publication media paperhi">
            <DIV class="img_box"><IMG class="papericon" src="figures/L2W-DEN.jpg"></DIV>
            <DIV class="media-body ">
                <A class="paper-title" href="https://www.arxiv.org/pdf/2209.08310.pdf"><B>Learning to Weight Samples for Dynamic Early-exiting Networks.</B></A> [<A href="https://github.com/LeapLabTHU/L2W-DEN"    target="_blank">code</A>][<A href="files/eccv2022_L2W-DEN.pdf"    target="_blank">slides</A>][<A href="files/4295.pdf"    target="_blank">poster</A>][<A href="files/4295.mp4"    target="_blank">video</A>]
                <br/><b>Yizeng Han*</b>, Yifan Pu*, Zihang Lai, Chaofei Wang, Shiji Song, Junfen Cao, Wenhui Huang, Chao Deng, Gao Huang.
                <br/><I>European Conference on Computer Vision (<b>ECCV</b>), 2022</I>
                <br/> In this paper, we propose to bridge the gap between training and testing of dynamic early-exiting networks by sample weighting. Intuitively, easy samples, which generally exit early in the network during inference, should contribute more to training early classifiers. The training of hard samples (mostly exit from deeper layers), however, should be emphasized by the late classifiers. Our work proposes to adopt a weight prediction network to weight the loss of different training samples at each exit. This weight prediction network and the backbone model are jointly optimized under a meta-learning framework with a novel optimization objective. By bringing the adaptive behavior during inference into the training phase, we show that the proposed weighting mechanism consistently improves the trade-off between classification accuracy and inference efficiency.
            </DIV>
            <DIV class="clear"></DIV>
        </DIV>

        <DIV class="publication media paperhi">
            <DIV class="img_box"><IMG class="papericon" src="figures/SAR_fig1.png"></DIV>
            <DIV class="media-body ">
                <A class="paper-title" href="papers/SAR.pdf"><B>Spatially Adaptive Feature Refinement for Efficient Inference.</B></A> 
                <br/><b>Yizeng Han</b>, Gao Huang, Shiji Song, Le Yang, Yitian Zhang, Haojun Jiang.
                <br/><I>IEEE Transactions on Image Processing (<b>TIP</b>, IF=11.041), 2021</I>
                <br/> We propose a novel Spatially Adaptive feature Refinement (SAR) approach to perform efficient inference by adaptively fusing information from two branches: one conducts standard convolution on input features at a lower spatial resolution, and the other one selectively refines a set of regions at the original resolution. The two branches complement each other in feature learning, and both of them evoke much less computation than standard convolution. Experiments on CIFAR and ImageNet classification, COCO object detection and PASCAL VOC semantic segmentation tasks validate that SAR can consistently improve the network performance and efficiency.
            </DIV>
            <DIV class="clear"></DIV>
        </DIV>


        <DIV class="publication media paperhi">
            <DIV class="img_box"><IMG class="papericon" src="figures/RANet.gif"></DIV>
            <DIV class="media-body ">
                <A class="paper-title" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Resolution_Adaptive_Networks_for_Efficient_Inference_CVPR_2020_paper.pdf"><B>Resolution Adaptive Networks for Efficient Inference.</B></A> [<A href="https://github.com/yangle15/RANet-pytorch"    target="_blank">code</A>]
                <br/>Le Yang*, <b>Yizeng Han*</b>, Xi Chen*, Shiji Song, Jifeng Dai, Gao Huang.
                <br/><I>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) 2020.</I>
                <br/>We focus on the spatial redundancy of images, and propose a novel Resolution Adaptive Network (RANet), which is inspired by the intuition that low-resolution representations are sufficient for classifying “easy” inputs, while only some “hard” samples need spatially detailed information. Empirically, we demonstrate the effectiveness of the proposed RANet on the CIFAR-10, CIFAR-100 and ImageNet datasets in both the anytime prediction setting and the budgeted batch classification setting.
            </DIV> 
            <DIV class="clear"></DIV>
        </DIV>

        <DIV class="publication media paperhi">
            <DIV class="img_box"><IMG class="papericon" src="figures/adafocus.png"></DIV>
            <DIV class="media-body ">
                <A class="paper-title" href="https://www.arxiv.org/pdf/2105.03245.pdf"><B>Adaptive Focus for Efficient Video Recognition.</B></A> [<A href="https://github.com/blackfeather-wang/AdaFocus"    target="_blank">code</A>]
                <br/>Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, <b>Yizeng Han</b>, and Gao Huang.
                <br/><I>IEEE/CVF International Conference on Computer Vision (<b>ICCV <font color="red">Oral</font></b>) 2021.</I>
                <br/>In this paper, we explore the spatial redundancy in video recognition with the aim to improve the computational efficiency. Extensive experiments on five benchmark datasets, i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is significantly more efficient than the competitive baselines.
            </DIV> 
            <DIV class="clear"></DIV>
        </DIV>

        <DIV class="publication media paperhi">
            <DIV class="img_box"><IMG class="papericon" src="figures/camloss.png"></DIV>
            <DIV class="media-body ">
                <A class="paper-title" href="https://arxiv.org/pdf/2109.01359.pdf"><B>Towards Learning Spatially Discriminative Feature Representations.</B></A> 
                <br/>Chaofei Wang*, Jiayu Xiao*, <b>Yizeng Han</b>, Qisen Yang, Shiji Song, Gao Huang.
                <br/><I>IEEE/CVF International Conference on Computer Vision (<b>ICCV</b>) 2021.</I><br/>We propose CAM-loss to constrain the embedded feature maps with the class activation maps (CAMs) which indicate the spatially discriminative regions of an image for particular categories. Experimental results show that CAM-loss is applicable to a variety of network structures and can be combined with mainstream regularization methods to improve the performance of image classification. The strong generalization ability of CAM-loss is validated in transfer learning and few shot learning tasks.
            </DIV> 
            <DIV class="clear"></DIV>
        </DIV>
    </DIV>
    <i>* Equal Contribution. </i>
    <br></DIV><br>
    <H2 id="awards" style="font-style:normal">Awards</H2>
    <DIV style="font-style:normal">
        <UL>
            <li>National Scholarship, Ministry of Education of China, 2022</li>
            <li>Comprehensive Merit Scholarship, 2017, 2016 at Tsinghua University.</li> 
            <li>Academic Excellence Scholarship, 2015 at Tsinghua University.</li> 
        </UL>
    </DIV>
 
    <H2 id="contact" style="font-style:normal">Contact</H2>
    <DIV style="font-style:normal">
        <UL>
            <LI> hanyz18 at mails dot tsinghua dot edu dot cn.</LI>
            <LI> 616 Centre Main Building, Tsinghua University, Beijing 100084, China.</LI>
        </UL>
    </DIV>
</DIV>
</BODY>
</HTML>
