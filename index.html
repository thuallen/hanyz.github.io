<!DOCTYPE HTML>
<!-- saved from url=(0034)http://www.gaohuang.net/ -->
<!DOCTYPE html PUBLIC "" ""><!-- Lab of GaoHuang from THU -->
<HTML lang="en-us"><HEAD><META content="IE=11.0000" http-equiv="X-UA-Compatible">
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<TITLE>Yizeng Han Homepage</TITLE>   
<LINK href="bootstrap.min.css" rel="stylesheet">  
<SCRIPT src="jquery-3.1.1.slim.min.js"></SCRIPT>

<STYLE type="text/css">
        /* @import url("http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600,600italic"); */

        body {
            /* font-family:"Roboto",Helvetica,Arial,sans-serif; */
            /* font-family: Arial, Helvetica, sans-serif; */
            /* font-family: "Times New Roman",Georgia,Serif; */
            font-family: "Helvetica", Helvetica, sans-serif;
            font-size: 16px;
            line-height: 1.5;
            font-weight: normal;
            background-color: #ffffff;
        }

        .navigation {
            height: auto;
            width: 100%;
            margin-left: 0;
            background: #8F4B8D;
            opacity: 0.9;
            position: fixed;
            top: 0;
        }

        .navigation ul {
            width: auto;
            list-style-type: none;
            white-space: nowrap;
            margin-left: 22%;
            padding: 0;
        }

        .navigation li {
            float: right;
            text-align: center;
            line-height: 40px;
            margin-right: 2%;
            position: relative;
            overflow: hidden;
        }

        #name-nav {
            float: left;
            position: relative;
            display: block;
            color: white;
            text-align: center;
            padding: 3px;
            overflow: hidden;
            font-size: 120%;
            text-decoration: none;
        }

        .navigation li a,
        .navigation li span {
            display: block;
            color: white;
            text-align: center;
            padding: 3px;
            overflow: hidden;
            text-decoration: none;
        }

        .navigation li a:hover {
            text-decoration: underline;
        }

        .navigation li span:hover {
            text-decoration: underline;
            cursor: pointer;
        }

        b {
            font-weight: 600;
        }

        .content {
            width: 100%;
            padding-top: 4%;
            /* margin : 0px auto; */
            background-color: #ffffff;
        }

        table {
            padding: 5px;
        }

        table.pub_table,
        td.pub_td1,
        td.pub_td2 {
            padding: 8px;
            width: 850px;
            border-collapse: separate;
            border-spacing: 15px;
            margin-top: -5px;
        }

        td.pub_td1 {
            width: 50px;
        }

        td.pub_td1 img {
            height: 120px;
            width: 160px;
        }

        div#container {
            padding-left: 20px;
            padding-right: 20px;
            margin-left: 15%;
            margin-right: 15%;
            /* width: 860px; */
            text-align: left;
            /* position: relative; */
            background-color: #FFF;
        }

        div#portrait {
            /* color: #1367a7;
        color: rgb(34, 110, 147); */
            height: 158px;
        }

        #portrait {
            /* position: relative; */
            /*margin-left: 100%;*/
        }

        h4,
        h3,
        h2,
        h1,
        .paperlo,
        .paperhi-only {
            color: rgb(153,87,157);
        }

        .text_container h2:hover {
            cursor: pointer;
        }

        .paperhi-only,
        .paperlo:hover {
            cursor: pointer;
        }

        h2 {
            font-size: 130%;
        }

        #paper-show {
            color: rgb(0, 85, 170);
            font-size: 80%;
        }

        #paper-show span:hover {
            text-decoration: underline;
        }

        /* p
	{
		color: #5B5B5B;
		margin-bottom: 50px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
	} */

        #header_img {
            position: absolute;
            top: 0px;
            right: 0px;
        }

        /* 
        a:link,
        a:visited {
            color: #1367a7;
            text-decoration: none;
        } */

        #mit_logo {
            position: absolute;
            left: 646px;
            top: 14px;
            width: 200px;
            height: 20px;
        }

        table.pub_table tr {
            outline: thin dotted #666666;
        }

        .paper-title {
            color: black;
            text-decoration: none;
        }

        .papericon {
            /* border-radius: 8px; */
            /* -moz-box-shadow: 3px 3px 6px #888;
            -webkit-box-shadow: 3px 3px 6px #888;
            box-shadow: 3px 3px 6px #888; */
            /*height: 120px;*/
            width: 160px;
            margin-top: auto;
            margin-left: 5px;
            margin-bottom: auto;
        }

        .media {
            margin-bottom: 15px;
            margin-left: 10px;
        }

        .media-body {
            margin-top: 5px;
            padding-left: 20px;
        }

        .publication {
            margin-bottom: 15px;
        }

        .papers-selected .publication {
            display: none;
        }

        .papers-selected .book-chapters {
            display: none;
        }

        .papers-selected #show-selected {
            color: black;
            text-decoration: underline;
        }

        .papers-selected .paperhi {
            display: flex;
        }

        .papers-selected .paper-year {
            display: none;
        }

        .papers-by-date #show-by-date {
            color: black;
            text-decoration: underline;
        }

        .papers-by-date .paper-selected {
            display: none;
        }

        .papers-by-date .book-chapters {
            display: none;
        }

        .book-chapters #book-chapters {
            color: black;
            text-decoration: underline;
        }

        .book-chapters .paper-selected,
        .book-chapters .paper-year,
        .book-chapters .publication {
            display: none;
        }

        .book-chapters .chapter {
            display: flex;
        }

        /* .papers-by-date .paperhi {
            display: none;
        } */

        .hidden>div {
            display: none;
        }

        .visible>div {
            display: block;
        }
</STYLE>
     
<SCRIPT>
        $(document).ready(function () {
            $('#show-selected').click(function () {
                $('.papers-container').removeClass('papers-by-date');
                $('.papers-container').removeClass('book-chapters');
                $('.papers-container').addClass('papers-selected');
            });

            $('#show-by-date').click(function () {
                $('.papers-container').removeClass('papers-selected');
                $('.papers-container').removeClass('book-chapters');
                $('.papers-container').addClass('papers-by-date');
            });

            $('#book-chapters').click(function () {
                $('.papers-container').removeClass('papers-selected');
                $('.papers-container').removeClass('papers-by-date');
                $('.papers-container').addClass('book-chapters');
            });

            $('.papers-container').addClass('papers-selected');


            $('.text_container').addClass("hidden");
            $('.text_container').click(function () {
                var $this = $(this);

                // if ($this.hasClass("hidden")) {
                //     $(this).removeClass("hidden").addClass("visible");

                // } else {
                //     $(this).removeClass("visible").addClass("hidden");
                // }
            });

            // document.querySelector("#news-nav").onclick = function () {
            //     document.querySelector("#news").scrollIntoView({
            //         block: "center",
            //         behavior: "smooth"
            //     });
            // }
            // publications
            document.querySelector("#publication-nav").onclick = function () {
                document.querySelector(".paperlo").scrollIntoView({
                    block: "center",
                    behavior: "smooth"
                });
            }
            // awards
            document.querySelector("#award-nav").onclick = function () {
                document.querySelector("#award").scrollIntoView({
                    block: "center",
                    behavior: "smooth"
                });
            }
            // talks
            document.querySelector("#talk-nav").onclick = function () {
                document.querySelector("#talk").scrollIntoView({
                    block: "center",
                    behavior: "smooth"
                });
            }
            // students
            document.querySelector("#student-nav").onclick = function () {
                document.querySelector("#student").scrollIntoView({
                    block: "center",
                    behavior: "smooth"
                });
            }
            // more
            document.querySelector("#more-nav").onclick = function () {
                document.querySelector("#more").scrollIntoView({
                    block: "center",
                    behavior: "smooth"
                });
            }

        });
</SCRIPT>
 
<META name="GENERATOR" content="MSHTML 11.00.10570.1001">

    <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?85819d0bd906cafe9ed67ce601c2e750";
          var s = document.getElementsByTagName("script")[0]; 
          s.parentNode.insertBefore(hm, s);
        })();
    </script>

</HEAD>

<BODY id="top">
<DIV class="navigation">
<UL>
<!--  <LI id="name-nav"></LI>
 <LI id="more-nav"><SPAN>More</SPAN></LI>
  <LI id="talk-nav"><SPAN>Talks</SPAN>
  <LI>
  <LI id="award-nav"><SPAN>Awards</SPAN>
  <LI> -->
  <li class="nav-item"><a href="#contact">Contact</a></li>
  <!-- <li class="nav-item"><a href="#talks">Talks</a></li> -->
  <!--<li class="nav-item"><a href="#students">Students</a></li> -->
  <li class="nav-item"><a href="#awards">Awards</a></li>
  <li class="nav-item"><a href="#publications">Publications</a></li>

  <li class="nav-item"><a href="#top">Home</a></li>

  <!-- <li class="nav-item"><a href="http://www.gaohuang.net/#teaching">Teaching</a></li> -->
  <!-- <li class="nav-item"><a href="http://www.gaohuang.net/#miscellaneous">Misc</a></li> -->

 <!-- <LI id="contact-nav"><SPAN>Contact</SPAN>
  <LI>
  <LI id="publication-nav"><SPAN>Publications</SPAN></LI>
  <LI><A href="http://www.gaohuang.net/#top">Home</A></LI> -->
</UL>
</DIV>

<DIV class="content">
<DIV id="container">
<TABLE width="100%">
  <TBODY>
  <TR>
    <TD width="60%">
      <DIV id="info" width="">
      <H1><br>Yizeng Han</H1><br>
      <P><B>Ph.D Candidate</B>, advised by Prof. <a href="http://www.gaohuang.net/">Gao Huang</a> and Prof. <a href="http://www.au.tsinghua.edu.cn/info/1075/1590.htm">Shiji Song</a>.<br>Department of Automation, Tsinghua University.</P>
      <h2>Education</h2>
      <Li>Ph.D, Tsinghua University, 2018 - present.</Li><Li>B.E., Tsinghua University, 2014 - 2018.</Li>
      <br>
    </DIV>
    </TD>
    <TD width="40%">
      <DIV id="photo" style="margin-left: 15%; float: right;"><IMG height="200" 
      id="portrait" src="figures/hyz.jpeg"></DIV>
    </TD>
  </TR>
  </TBODY>
</TABLE>

<h2>Research Experience</h2>
    <li>Intern, Georgia Institute of Technology, 06/2017 - 08/2017</li>
    <br>

<h2>Research Interest</h2>
    My research focuses on machine learning and computer vision, in particular deep learning, eﬃcient inference and dynamic neural networks.
    <br>
    <br>

<a name="publications"></a>
<DIV class="papers-container">
    <H2 class="paperlo">Recent Publications &amp; Preprints (<a href="https://scholar.google.com/citations?user=25mubAsAAAAJ&hl=en">Google Scholar</a>)
        <br>
        <SPAN id="paper-show">
            <!-- (<SPAN id="show-selected">show selected</SPAN> /  
                <SPAN id="show-by-date">show all by date</SPAN> / 
                <SPAN id="book-chapters">book chapters</SPAN>) -->
        </SPAN>       
    </H2><!-- <h5 class="paperhi paperhi-only">Representative Publications</h5> --> 

    <DIV class="paper-selected">  
        <!-- (<a href="https://scholar.google.com/citations?user=-P9LwcgAAAAJ&amp;hl=en">Full publication list on Google Scholar</a>) -->
        <br>
        <DIV class="publication media paperhi">
            <IMG class="papericon" src="figures/survey.png">
            <DIV class="media-body ">
                <A class="paper-title" href="https://arxiv.org/pdf/2102.04906.pdf"><B>Dynamic Neural Networks: A Survey.</B></A> [<A href="https://mp.weixin.qq.com/s/TG_HBAR7Jrec4X02sxNGEA"    target="_blank">智源社区</A>][<A href="https://jmq.h5.xeknow.com/s/2H6ZSj">机器之心-在线讲座</A>][<A href="https://www.bilibili.com/video/BV19B4y1A7Wy?from=search&seid=12254026542403915477">Bilibili</A>]
                <br/><b>Yizeng Han</b>*, Gao Huang*, Shiji Song, Le Yang, Honghui Wang, Yulin Wang.
                <br/><I>Arxiv Preprint.</I>
                <br/>Dynamic neural network is an emerging research topic in deep learning. Compared to static models which have fixed computational graphs and parameters at the inference stage, dynamic networks can adapt their structures or parameters to different inputs, leading to notable advantages in terms of accuracy, computational efficiency, adaptiveness, etc. In this survey, we comprehensively review this rapidly developing area by dividing dynamic networks into three main categories: 1) sample-wise dynamic models that process each sample with data-dependent architectures or parameters; 2) spatial-wise dynamic networks that conduct adaptive computation with respect to different spatial locations of image data; and 3) temporal-wise dynamic models that perform adaptive inference along the temporal dimension for sequential data such as videos and texts. The important research problems of dynamic networks, e.g., architecture design, decision making scheme, optimization technique and applications, are reviewed systematically. Finally, we discuss the open problems in this field together with interesting future research directions.
            </DIV>
        </DIV>

        <DIV class="publication media paperhi">
            <IMG class="papericon" src="figures/RANet.gif">
            <DIV class="media-body ">
                <A class="paper-title" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_Resolution_Adaptive_Networks_for_Efficient_Inference_CVPR_2020_paper.pdf"><B>Resolution Adaptive Networks for Efficient Inference.</B></A> [<A href="https://github.com/yangle15/RANet-pytorch"    target="_blank">code</A>]
                <br/>Le Yang*, <b>Yizeng Han*</b>, Xi Chen*, Shiji Song, Jifeng Dai, Gao Huang.
                <br/><I>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR </b>) 2020.</I>
                <br/>Adaptive inference is an effective mechanism to achieve a dynamic tradeoff between accuracy and computational cost in deep networks. Existing works mainly exploit architecture redundancy in network depth or width. In this paper, we focus on spatial redundancy of input samples and propose a novel Resolution Adaptive Network (RANet), which is inspired by the intuition that low-resolution representations are sufficient for classifying “easy” inputs containing large objects with prototypical features, while only some “hard” samples need spatially detailed information. In RANet, the input images are first routed to a lightweight sub-network that efficiently extracts low-resolution representations, and those samples with high prediction confidence will exit early from the network without being further processed. Meanwhile, high-resolution paths in the network maintain the capability to recognize the “hard” samples. Therefore, RANet can effectively reduce the spatial redundancy involved in inferring high-resolution input- s. Empirically, we demonstrate the effectiveness of the pro- posed RANet on the CIFAR-10, CIFAR-100 and ImageNet datasets in both the anytime prediction setting and the budgeted batch classification setting.
            </DIV> 
        </DIV>

        <DIV class="publication media paperhi">
            <IMG class="papericon" src="figures/adafocus.png">
            <DIV class="media-body ">
                <A class="paper-title" href="https://arxiv.org/pdf/2105.03245.pdf"><B>Adaptive Focus for Efficient Video Recognition.</B></A>
                <br/>Yulin Wang, Zhaoxi Chen, Haojun Jiang, Shiji Song, <b>Yizeng Han</b>, and Gao Huang.
                <br/><I>Arxiv Preprint.</I>
                <br/>In this paper, we explore the spatial redundancy in video recognition with the aim to improve the computational efficiency. Extensive experiments on five benchmark datasets, i.e., ActivityNet, FCVID, Mini-Kinetics, Something-Something V1&V2, demonstrate that our method is significantly more efficient than the competitive baselines.
            </DIV> 
        </DIV>
    </DIV>
    <i>* Equal Contribution. </i>
    <br></DIV><br>
    <H2 id="awards" style="font-style:normal">Awards</H2>
    <DIV style="font-style:normal">
        <UL>
            <li>Comprehensive Merit Scholarship, 2017, 2016 at Tsinghua University.</li> 
            <li>Academic Excellence Scholarship, 2015 at Tsinghua University.</li> 
        </UL>
    </DIV>
 
    <H2 id="contact" style="font-style:normal">Contact</H2>
    <DIV style="font-style:normal">
        <UL>
            <LI> hanyz18 at mails dot tsinghua dot edu dot cn.</LI>
            <LI> 616 Centre Main Building, Tsinghua University, Beijing 100084, China.</LI>
        </UL>
    </DIV>
</DIV>
</BODY>
</HTML>
